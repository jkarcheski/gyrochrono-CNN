{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b84aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250909 14:12:11 utils:160] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gyrointerp\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18217059-226c-4310-9788-cf2dce1f3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(epoch, model, opt, loss):\n",
    "    torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": opt.state_dict(),\n",
    "    \"loss\": loss.item(),\n",
    "    }, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a34f44-9d41-4b6b-bd1d-4927bc445673",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4684e07f-c9e2-4497-99f3-602ca2e69659",
   "metadata": {},
   "outputs": [],
   "source": [
    "teffs = np.linspace(3800, 6200, 100)\n",
    "teffs_n = teffs + np.random.normal(0, 75, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8f865c-e1b5-4f50-8240-d4d057d95081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prots=gyrointerp.models.slow_sequence(teffs_n, 100)\n",
    "prots_n=gyrointerp.models.slow_sequence(teffs_n, 100)+ np.random.normal(0, gyrointerp.models.slow_sequence(teffs_n, 100) * .1, 100)\n",
    "plt.scatter(teffs_n,prots_n)\n",
    "plt.gca().invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d974a2-8207-4bf8-ac1b-c449e8f11e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gyro_data(ages, teff_err=75, prot_err=0.1, nsize=100, plot=False):\n",
    "    all_data = []  # to hold arrays of shape (nsize, 2) for each age\n",
    "\n",
    "    for age in ages:\n",
    "        # Generate teff and noisy teff\n",
    "        teffs = np.linspace(3800, 6200, nsize)\n",
    "        teffs_n = teffs + np.random.normal(0, teff_err, nsize)\n",
    "        teffs_n = np.clip(teffs_n, 3800, 6200)\n",
    "\n",
    "        # Generate rotation periods from gyro model\n",
    "        prots = gyrointerp.models.slow_sequence(teffs, age)\n",
    "        prots_n = gyrointerp.models.slow_sequence(teffs_n, age) + np.random.normal(\n",
    "            0, gyrointerp.models.slow_sequence(teffs_n, age) * prot_err, nsize\n",
    "        )\n",
    "\n",
    "        # Optionally show scatter plot\n",
    "        if plot:\n",
    "            plt.scatter(teffs_n, prots_n)\n",
    "            plt.gca().invert_xaxis()\n",
    "            plt.title(f'Age = {age}')\n",
    "            plt.xlabel('Teff (K)')\n",
    "            plt.ylabel('Prot (days)')\n",
    "            plt.show()\n",
    "\n",
    "        # Stack teff and prot into shape (nsize, 2)\n",
    "        data = np.column_stack((teffs_n, prots_n))\n",
    "        all_data.append(data)\n",
    "\n",
    "    return all_data, ages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58116-bc3f-4fa6-af28-b5999409701b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2357f3b-7981-46ff-9679-7bfd36199a70",
   "metadata": {},
   "source": [
    "Step 1: Load the data\n",
    "> right now I'm using some very basic proxy data. replace this with your own data.\n",
    "> \n",
    "> I'm assuming a format of input X -> [Teff, Prot] and output y -> [age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38c74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, age  = gyro_data(np.random.uniform(100,1000,200))\n",
    "#mask = (data[:, 1] > 0) & (data[:, 1] < 60) & (data[:, 0] > 3800) & (data[:, 0] < 6200)\n",
    "#data_mask=data[mask]\n",
    "#age_mask=age[mask]\n",
    "def load_files():\n",
    "    # load your csv file into a numpy array\n",
    "    # make sure length of X is the same as the length of y \n",
    "    X = np.array(data)\n",
    "    y = np.array(age)\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9300d-b838-4ca9-a59b-90544c2dc874",
   "metadata": {},
   "source": [
    "Step 2: Normalize the data\n",
    "> Again, rewrite to fit your needs.\n",
    "> \n",
    "> I recommend to ensure that your normalization worked by plotting a histogram of the normalized data. You want the distribution to range from 0.0 to 1.0 but also it should be fairly distributed.\n",
    "> \n",
    "> I also recommend that you save (print out) the values you use to normalize the data (min, max, etc.). Remember that the transformation needs to be invertible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620083e7-90e9-47b9-853a-2e2d86e4e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_og(x):\n",
    "    # x = np.arctan(x) \n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d6287d-16df-4dde-90f0-68451429cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x2=np.log10(x)\n",
    "    x_std=x2\n",
    "    x_std=(x2 - np.min(x2)) / (np.max(x2) - np.min(x2))\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f6124-3f0a-48b5-b8d9-cba6ea10c028",
   "metadata": {},
   "source": [
    "Step 3: Prepare and split the datasets\n",
    "> typically, people split the data into three main sets: TRAIN, VALIDATION, and TEST\n",
    ">> TRAIN is (you guessed it!) for training. This means tuning the actual parameters (weights and biases) INSIDE of the model. \n",
    "\n",
    ">> VALIDATION is for hyperparameter tuning. hyperparameters are all of the things you can tune OUTSIDE of the model like learning rate, batch size, model depth/number of layers, etc\n",
    "\n",
    ">> TEST is for after training is completed.  eventually, we can use the ursa major group as a test set. this is the real \"make it or break it\" test on data that the model has never seen before. this is important because the model could be 'overfit' to the training data.  \n",
    "\n",
    "> right now, I'm splitting the data so that 80% goes to TRAIN. then, from the remaining 20%, I take half for VALIDATION and half for TEST. so we have an 80/10/10 split. you can play around with this, just make sure there's at least a couple hundred samples in each data subset.\n",
    " \n",
    "> some people use only train/test which is fine. if you do it this way, an 80/20 split is usually good. \n",
    "\n",
    "> Note about batch size: the smaller the better but you don't want it to be too small because it will slow down your computer a lot. 32 or 64 is usually ideal, but you can push even lower if your computer allows.\n",
    "\n",
    "> Also: if you change the seed, your splits will change. I recommend saving the data tensors or arrays separately to their own csv files or something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1cf1bd-8f42-47bb-8d3e-03db68d23977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(all_data, all_labels, size=0.2, seed=42, batch_size=64):\n",
    "    \n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(all_data, all_labels, test_size=size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.double)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.double) \n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.double)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.double)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.double)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.double)\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create PyTorch DataLoader instances\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce27e3-53d0-4183-ae57-e664f11129fd",
   "metadata": {},
   "source": [
    "Step 5: Create the neural net architecture\n",
    "> You shouldn't have to tweak this much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4f5ee8-f2ac-4393-a932-8af5f2492f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StellarClusterCNN(nn.Module):\n",
    "    def __init__(self):  # <-- corrected here\n",
    "        super(StellarClusterCNN, self).__init__()  # <-- and here\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff773d-1974-434a-80fe-3e2db48e2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_loader, val_loader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    # Print the model architecture\n",
    "    # print(model)\n",
    "    \n",
    "    # training loop:\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float() \n",
    "        \n",
    "            # important! must always zero out the gradients before the next forward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            outputs = model(inputs).squeeze() \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%100==0:\n",
    "            print(\"epoch:\", epoch)\n",
    "            eval(model, val_loader)\n",
    "            print(\"Loss: \", loss.item())\n",
    "            save_state(epoch, model, optimizer, loss)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90a121-b109-457e-b8a4-316a045b4b76",
   "metadata": {},
   "source": [
    "Step _: Run everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61dde79-15c7-463d-8c4f-1484a3db5630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data\n",
      "creating a new model\n",
      "starting training\n",
      "epoch: 0\n",
      "Validation MSE: 0.503298\n",
      "Loss:  0.4110347628593445\n",
      "epoch: 100\n",
      "Validation MSE: 0.500390\n",
      "Loss:  0.4529581665992737\n",
      "epoch: 200\n",
      "Validation MSE: 0.497514\n",
      "Loss:  0.4803321957588196\n",
      "epoch: 300\n",
      "Validation MSE: 0.494697\n",
      "Loss:  0.5742403864860535\n",
      "epoch: 400\n",
      "Validation MSE: 0.491971\n",
      "Loss:  0.4184856712818146\n",
      "epoch: 500\n",
      "Validation MSE: 0.489259\n",
      "Loss:  0.47992730140686035\n",
      "epoch: 600\n",
      "Validation MSE: 0.486547\n",
      "Loss:  0.5830190181732178\n",
      "epoch: 700\n",
      "Validation MSE: 0.483828\n",
      "Loss:  0.5756881833076477\n",
      "epoch: 800\n",
      "Validation MSE: 0.481095\n",
      "Loss:  0.4805285334587097\n",
      "epoch: 900\n",
      "Validation MSE: 0.478340\n",
      "Loss:  0.40367591381073\n",
      "epoch: 1000\n",
      "Validation MSE: 0.475556\n",
      "Loss:  0.629343569278717\n",
      "epoch: 1100\n",
      "Validation MSE: 0.472760\n",
      "Loss:  0.4280792474746704\n",
      "epoch: 1200\n",
      "Validation MSE: 0.469970\n",
      "Loss:  0.5736992359161377\n",
      "epoch: 1300\n",
      "Validation MSE: 0.467249\n",
      "Loss:  0.44971963763237\n",
      "epoch: 1400\n",
      "Validation MSE: 0.464491\n",
      "Loss:  0.4661926031112671\n",
      "epoch: 1500\n",
      "Validation MSE: 0.461676\n",
      "Loss:  0.47042766213417053\n",
      "epoch: 1600\n",
      "Validation MSE: 0.458697\n",
      "Loss:  0.4643942713737488\n",
      "epoch: 1700\n",
      "Validation MSE: 0.455733\n",
      "Loss:  0.37548431754112244\n",
      "epoch: 1800\n",
      "Validation MSE: 0.452721\n",
      "Loss:  0.6078281998634338\n",
      "epoch: 1900\n",
      "Validation MSE: 0.449722\n",
      "Loss:  0.44084256887435913\n",
      "epoch: 2000\n",
      "Validation MSE: 0.446723\n",
      "Loss:  0.4658718407154083\n",
      "epoch: 2100\n",
      "Validation MSE: 0.443695\n",
      "Loss:  0.41970524191856384\n",
      "epoch: 2200\n",
      "Validation MSE: 0.440698\n",
      "Loss:  0.5662739276885986\n",
      "epoch: 2300\n",
      "Validation MSE: 0.437687\n",
      "Loss:  0.4145931303501129\n",
      "epoch: 2400\n",
      "Validation MSE: 0.434656\n",
      "Loss:  0.36935630440711975\n",
      "epoch: 2500\n",
      "Validation MSE: 0.431631\n",
      "Loss:  0.40279650688171387\n",
      "epoch: 2600\n",
      "Validation MSE: 0.428599\n",
      "Loss:  0.4835126996040344\n",
      "epoch: 2700\n",
      "Validation MSE: 0.425568\n",
      "Loss:  0.46872544288635254\n",
      "epoch: 2800\n",
      "Validation MSE: 0.422477\n",
      "Loss:  0.47567427158355713\n",
      "epoch: 2900\n",
      "Validation MSE: 0.419397\n",
      "Loss:  0.40760189294815063\n",
      "epoch: 3000\n",
      "Validation MSE: 0.416268\n",
      "Loss:  0.3878944516181946\n",
      "epoch: 3100\n",
      "Validation MSE: 0.413135\n",
      "Loss:  0.41767096519470215\n",
      "epoch: 3200\n",
      "Validation MSE: 0.409989\n",
      "Loss:  0.3892112374305725\n",
      "epoch: 3300\n",
      "Validation MSE: 0.406829\n",
      "Loss:  0.4245228171348572\n",
      "epoch: 3400\n",
      "Validation MSE: 0.403622\n",
      "Loss:  0.3636254668235779\n",
      "epoch: 3500\n",
      "Validation MSE: 0.400241\n",
      "Loss:  0.4729197919368744\n",
      "epoch: 3600\n",
      "Validation MSE: 0.396909\n",
      "Loss:  0.3858402669429779\n",
      "epoch: 3700\n",
      "Validation MSE: 0.393598\n",
      "Loss:  0.4450986981391907\n",
      "epoch: 3800\n",
      "Validation MSE: 0.390280\n",
      "Loss:  0.376076877117157\n",
      "epoch: 3900\n",
      "Validation MSE: 0.386945\n",
      "Loss:  0.34628134965896606\n",
      "epoch: 4000\n",
      "Validation MSE: 0.383573\n",
      "Loss:  0.45271357893943787\n",
      "epoch: 4100\n",
      "Validation MSE: 0.380158\n",
      "Loss:  0.4005504250526428\n",
      "epoch: 4200\n",
      "Validation MSE: 0.376745\n",
      "Loss:  0.4156581163406372\n",
      "epoch: 4300\n",
      "Validation MSE: 0.373310\n",
      "Loss:  0.48705345392227173\n",
      "epoch: 4400\n",
      "Validation MSE: 0.369797\n",
      "Loss:  0.41137316823005676\n",
      "epoch: 4500\n",
      "Validation MSE: 0.366290\n",
      "Loss:  0.37225016951560974\n",
      "epoch: 4600\n",
      "Validation MSE: 0.362767\n",
      "Loss:  0.34303519129753113\n",
      "epoch: 4700\n",
      "Validation MSE: 0.359195\n",
      "Loss:  0.37894266843795776\n",
      "epoch: 4800\n",
      "Validation MSE: 0.355597\n",
      "Loss:  0.41285398602485657\n",
      "epoch: 4900\n",
      "Validation MSE: 0.351980\n",
      "Loss:  0.369515597820282\n",
      "epoch: 5000\n",
      "Validation MSE: 0.348299\n",
      "Loss:  0.4639419615268707\n",
      "epoch: 5100\n",
      "Validation MSE: 0.344634\n",
      "Loss:  0.3659067153930664\n",
      "epoch: 5200\n",
      "Validation MSE: 0.340946\n",
      "Loss:  0.3688599765300751\n",
      "epoch: 5300\n",
      "Validation MSE: 0.337304\n",
      "Loss:  0.3691503703594208\n",
      "epoch: 5400\n",
      "Validation MSE: 0.333470\n",
      "Loss:  0.4203542470932007\n",
      "epoch: 5500\n",
      "Validation MSE: 0.329668\n",
      "Loss:  0.30394116044044495\n",
      "epoch: 5600\n",
      "Validation MSE: 0.325817\n",
      "Loss:  0.3937141001224518\n",
      "epoch: 5700\n",
      "Validation MSE: 0.321980\n",
      "Loss:  0.47240468859672546\n",
      "epoch: 5800\n",
      "Validation MSE: 0.318126\n",
      "Loss:  0.38909247517585754\n",
      "epoch: 5900\n",
      "Validation MSE: 0.314272\n",
      "Loss:  0.3170691728591919\n",
      "epoch: 6000\n",
      "Validation MSE: 0.310429\n",
      "Loss:  0.3755017817020416\n",
      "epoch: 6100\n",
      "Validation MSE: 0.306624\n",
      "Loss:  0.3507758677005768\n",
      "epoch: 6200\n",
      "Validation MSE: 0.302761\n",
      "Loss:  0.37554776668548584\n",
      "epoch: 6300\n",
      "Validation MSE: 0.298891\n",
      "Loss:  0.3232508599758148\n",
      "epoch: 6400\n",
      "Validation MSE: 0.295018\n",
      "Loss:  0.3077237606048584\n",
      "epoch: 6500\n",
      "Validation MSE: 0.291119\n",
      "Loss:  0.31229960918426514\n",
      "epoch: 6600\n",
      "Validation MSE: 0.287199\n",
      "Loss:  0.2659713327884674\n",
      "epoch: 6700\n",
      "Validation MSE: 0.283314\n",
      "Loss:  0.26123738288879395\n",
      "epoch: 6800\n",
      "Validation MSE: 0.279437\n",
      "Loss:  0.36725711822509766\n",
      "epoch: 6900\n",
      "Validation MSE: 0.275544\n",
      "Loss:  0.23649045825004578\n",
      "epoch: 7000\n",
      "Validation MSE: 0.271643\n",
      "Loss:  0.22791452705860138\n",
      "epoch: 7100\n",
      "Validation MSE: 0.267751\n",
      "Loss:  0.30889976024627686\n",
      "epoch: 7200\n",
      "Validation MSE: 0.263814\n",
      "Loss:  0.25556913018226624\n",
      "epoch: 7300\n",
      "Validation MSE: 0.259896\n",
      "Loss:  0.29933249950408936\n",
      "epoch: 7400\n",
      "Validation MSE: 0.255928\n",
      "Loss:  0.29143357276916504\n",
      "epoch: 7500\n",
      "Validation MSE: 0.252007\n",
      "Loss:  0.281852126121521\n",
      "epoch: 7600\n",
      "Validation MSE: 0.248120\n",
      "Loss:  0.21535082161426544\n",
      "epoch: 7700\n",
      "Validation MSE: 0.244185\n",
      "Loss:  0.2378215342760086\n",
      "epoch: 7800\n",
      "Validation MSE: 0.240235\n",
      "Loss:  0.3077796697616577\n",
      "epoch: 7900\n",
      "Validation MSE: 0.236287\n",
      "Loss:  0.2659081816673279\n",
      "epoch: 8000\n",
      "Validation MSE: 0.232320\n",
      "Loss:  0.288802832365036\n",
      "epoch: 8100\n",
      "Validation MSE: 0.228360\n",
      "Loss:  0.2407606542110443\n",
      "epoch: 8200\n",
      "Validation MSE: 0.224425\n",
      "Loss:  0.2502581477165222\n",
      "epoch: 8300\n",
      "Validation MSE: 0.220529\n",
      "Loss:  0.17909874022006989\n",
      "epoch: 8400\n",
      "Validation MSE: 0.216621\n",
      "Loss:  0.31208547949790955\n",
      "epoch: 8500\n",
      "Validation MSE: 0.212673\n",
      "Loss:  0.24263426661491394\n",
      "epoch: 8600\n",
      "Validation MSE: 0.208790\n",
      "Loss:  0.1950525939464569\n",
      "epoch: 8700\n",
      "Validation MSE: 0.204949\n",
      "Loss:  0.17762993276119232\n",
      "epoch: 8800\n",
      "Validation MSE: 0.201146\n",
      "Loss:  0.20605120062828064\n",
      "epoch: 8900\n",
      "Validation MSE: 0.197359\n",
      "Loss:  0.28633105754852295\n",
      "epoch: 9000\n",
      "Validation MSE: 0.193470\n",
      "Loss:  0.16759127378463745\n",
      "epoch: 9100\n",
      "Validation MSE: 0.189644\n",
      "Loss:  0.21384121477603912\n",
      "epoch: 9200\n",
      "Validation MSE: 0.185804\n",
      "Loss:  0.2226601243019104\n",
      "epoch: 9300\n",
      "Validation MSE: 0.182031\n",
      "Loss:  0.20518262684345245\n",
      "epoch: 9400\n",
      "Validation MSE: 0.178274\n",
      "Loss:  0.18821397423744202\n",
      "epoch: 9500\n",
      "Validation MSE: 0.174557\n",
      "Loss:  0.17486520111560822\n",
      "epoch: 9600\n",
      "Validation MSE: 0.170940\n",
      "Loss:  0.15838423371315002\n",
      "epoch: 9700\n",
      "Validation MSE: 0.167285\n",
      "Loss:  0.21503210067749023\n",
      "epoch: 9800\n",
      "Validation MSE: 0.163669\n",
      "Loss:  0.18669521808624268\n",
      "epoch: 9900\n",
      "Validation MSE: 0.160160\n",
      "Loss:  0.154753640294075\n",
      "training complete. \n",
      "starting post-training evaluation.\n",
      "Validation MSE: 0.169358\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "num_epochs = 10000\n",
    "\n",
    "print(\"loading in data\")\n",
    "\n",
    "# Load in data\n",
    "raw_X, raw_y = load_files()\n",
    "\n",
    "# Normalize data\n",
    "normalized_X = np.swapaxes(np.array((normalize(raw_X[:,0]), normalize(raw_X[:,1]))), 0, 1) ## REPLACE WITH YOUR OWN IMPLEMENTATION\n",
    "normalized_y = np.array(normalize(raw_y.squeeze())) ## REPLACE \n",
    "\n",
    "# Split data. Make sure your data is in this format... X shape: (# samples, # dims)  y shape: (# samples ,)\n",
    "train_loader, test_loader, val_loader = split_dataset(normalized_X, normalized_y)\n",
    "\n",
    "print(\"creating a new model\")\n",
    "# Create an instance of the model\n",
    "model = StellarClusterCNN()\n",
    "\n",
    "\n",
    "print(\"starting training\")\n",
    "train(model, num_epochs, train_loader, val_loader)\n",
    "\n",
    "print(\"training complete. \\nstarting post-training evaluation.\")\n",
    "eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca55a3b-a941-412c-9e5d-f73aa2d3bf37",
   "metadata": {},
   "source": [
    "Step âˆž: (things I haven't implemented yet)\n",
    "> 1. Saving the model/checkpoints\n",
    "> 2. Visualizing the loss over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbde66c0-01df-4b13-b7eb-f41773f57a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StellarClusterCNN(\n",
       "  (conv1): Conv1d(2, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (relu3): ReLU()\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading in a checkpoint\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "model = StellarClusterCNN()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)   # TODO: change lr\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "start_epoch = checkpoint[\"epoch\"]\n",
    "loss_at_save = checkpoint[\"loss\"]\n",
    "\n",
    "model.train()  # Switch back to training mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ea28b9-e063-4d43-a278-a1e5be535f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Validation MSE: 0.160123\n",
      "Loss:  0.1373797059059143\n",
      "epoch: 100\n",
      "Validation MSE: 0.156495\n",
      "Loss:  0.20538866519927979\n",
      "epoch: 200\n",
      "Validation MSE: 0.152908\n",
      "Loss:  0.16237971186637878\n",
      "epoch: 300\n",
      "Validation MSE: 0.149424\n",
      "Loss:  0.1525651514530182\n",
      "epoch: 400\n",
      "Validation MSE: 0.146006\n",
      "Loss:  0.19373659789562225\n",
      "epoch: 500\n",
      "Validation MSE: 0.142584\n",
      "Loss:  0.14183339476585388\n",
      "epoch: 600\n",
      "Validation MSE: 0.139260\n",
      "Loss:  0.1362534612417221\n",
      "epoch: 700\n",
      "Validation MSE: 0.135971\n",
      "Loss:  0.12178321927785873\n",
      "epoch: 800\n",
      "Validation MSE: 0.132725\n",
      "Loss:  0.12034965306520462\n",
      "epoch: 900\n",
      "Validation MSE: 0.129604\n",
      "Loss:  0.16102559864521027\n",
      "epoch: 1000\n",
      "Validation MSE: 0.126518\n",
      "Loss:  0.10612967610359192\n",
      "epoch: 1100\n",
      "Validation MSE: 0.123430\n",
      "Loss:  0.1401403844356537\n",
      "epoch: 1200\n",
      "Validation MSE: 0.120492\n",
      "Loss:  0.14590895175933838\n",
      "epoch: 1300\n",
      "Validation MSE: 0.117634\n",
      "Loss:  0.1642199456691742\n",
      "epoch: 1400\n",
      "Validation MSE: 0.114868\n",
      "Loss:  0.15573705732822418\n",
      "epoch: 1500\n",
      "Validation MSE: 0.112218\n",
      "Loss:  0.11832626163959503\n",
      "epoch: 1600\n",
      "Validation MSE: 0.109636\n",
      "Loss:  0.1328943520784378\n",
      "epoch: 1700\n",
      "Validation MSE: 0.107056\n",
      "Loss:  0.170576274394989\n",
      "epoch: 1800\n",
      "Validation MSE: 0.104589\n",
      "Loss:  0.12526074051856995\n",
      "epoch: 1900\n",
      "Validation MSE: 0.102119\n",
      "Loss:  0.11119745671749115\n",
      "epoch: 2000\n",
      "Validation MSE: 0.099771\n",
      "Loss:  0.09263797104358673\n",
      "epoch: 2100\n",
      "Validation MSE: 0.097560\n",
      "Loss:  0.08074583113193512\n",
      "epoch: 2200\n",
      "Validation MSE: 0.095405\n",
      "Loss:  0.1412685215473175\n",
      "epoch: 2300\n",
      "Validation MSE: 0.093334\n",
      "Loss:  0.11788877844810486\n",
      "epoch: 2400\n",
      "Validation MSE: 0.091355\n",
      "Loss:  0.10992783308029175\n",
      "epoch: 2500\n",
      "Validation MSE: 0.089408\n",
      "Loss:  0.09849133342504501\n",
      "epoch: 2600\n",
      "Validation MSE: 0.087680\n",
      "Loss:  0.10550367832183838\n",
      "epoch: 2700\n",
      "Validation MSE: 0.085936\n",
      "Loss:  0.13389578461647034\n",
      "epoch: 2800\n",
      "Validation MSE: 0.084363\n",
      "Loss:  0.10555380582809448\n",
      "epoch: 2900\n",
      "Validation MSE: 0.082871\n",
      "Loss:  0.09766550362110138\n",
      "epoch: 3000\n",
      "Validation MSE: 0.081482\n",
      "Loss:  0.1133095845580101\n",
      "epoch: 3100\n",
      "Validation MSE: 0.080182\n",
      "Loss:  0.08727984875440598\n",
      "epoch: 3200\n",
      "Validation MSE: 0.078973\n",
      "Loss:  0.1318661868572235\n",
      "epoch: 3300\n",
      "Validation MSE: 0.077859\n",
      "Loss:  0.12869934737682343\n",
      "epoch: 3400\n",
      "Validation MSE: 0.076843\n",
      "Loss:  0.07377755641937256\n",
      "epoch: 3500\n",
      "Validation MSE: 0.075917\n",
      "Loss:  0.11279429495334625\n",
      "epoch: 3600\n",
      "Validation MSE: 0.075026\n",
      "Loss:  0.09324562549591064\n",
      "epoch: 3700\n",
      "Validation MSE: 0.074233\n",
      "Loss:  0.11683815717697144\n",
      "epoch: 3800\n",
      "Validation MSE: 0.073471\n",
      "Loss:  0.13101209700107574\n",
      "epoch: 3900\n",
      "Validation MSE: 0.072895\n",
      "Loss:  0.1163727268576622\n",
      "epoch: 4000\n",
      "Validation MSE: 0.072345\n",
      "Loss:  0.12174001336097717\n",
      "epoch: 4100\n",
      "Validation MSE: 0.071874\n",
      "Loss:  0.09948265552520752\n",
      "epoch: 4200\n",
      "Validation MSE: 0.071452\n",
      "Loss:  0.08979934453964233\n",
      "epoch: 4300\n",
      "Validation MSE: 0.071099\n",
      "Loss:  0.09652626514434814\n",
      "epoch: 4400\n",
      "Validation MSE: 0.070793\n",
      "Loss:  0.12339285016059875\n",
      "epoch: 4500\n",
      "Validation MSE: 0.070573\n",
      "Loss:  0.08996500074863434\n",
      "epoch: 4600\n",
      "Validation MSE: 0.070384\n",
      "Loss:  0.11055658757686615\n",
      "epoch: 4700\n",
      "Validation MSE: 0.070203\n",
      "Loss:  0.08352044969797134\n",
      "epoch: 4800\n",
      "Validation MSE: 0.070067\n",
      "Loss:  0.09199512004852295\n",
      "epoch: 4900\n",
      "Validation MSE: 0.069940\n",
      "Loss:  0.07347457855939865\n",
      "epoch: 5000\n",
      "Validation MSE: 0.069824\n",
      "Loss:  0.056497834622859955\n",
      "epoch: 5100\n",
      "Validation MSE: 0.069707\n",
      "Loss:  0.06845857203006744\n",
      "epoch: 5200\n",
      "Validation MSE: 0.069674\n",
      "Loss:  0.1027527004480362\n",
      "epoch: 5300\n",
      "Validation MSE: 0.069615\n",
      "Loss:  0.09477176517248154\n",
      "epoch: 5400\n",
      "Validation MSE: 0.069565\n",
      "Loss:  0.11192258447408676\n",
      "epoch: 5500\n",
      "Validation MSE: 0.069532\n",
      "Loss:  0.06908179074525833\n",
      "epoch: 5600\n",
      "Validation MSE: 0.069494\n",
      "Loss:  0.08841541409492493\n",
      "epoch: 5700\n",
      "Validation MSE: 0.069470\n",
      "Loss:  0.08804912120103836\n",
      "epoch: 5800\n",
      "Validation MSE: 0.069438\n",
      "Loss:  0.0806787833571434\n",
      "epoch: 5900\n",
      "Validation MSE: 0.069434\n",
      "Loss:  0.08028280735015869\n",
      "epoch: 6000\n",
      "Validation MSE: 0.069419\n",
      "Loss:  0.07315152138471603\n",
      "epoch: 6100\n",
      "Validation MSE: 0.069397\n",
      "Loss:  0.08519594371318817\n",
      "epoch: 6200\n",
      "Validation MSE: 0.069384\n",
      "Loss:  0.0942637175321579\n",
      "epoch: 6300\n",
      "Validation MSE: 0.069369\n",
      "Loss:  0.09675168991088867\n",
      "epoch: 6400\n",
      "Validation MSE: 0.069365\n",
      "Loss:  0.08198182284832001\n",
      "epoch: 6500\n",
      "Validation MSE: 0.069356\n",
      "Loss:  0.08934345096349716\n",
      "epoch: 6600\n",
      "Validation MSE: 0.069344\n",
      "Loss:  0.10295800119638443\n",
      "epoch: 6700\n",
      "Validation MSE: 0.069324\n",
      "Loss:  0.08247289806604385\n",
      "epoch: 6800\n",
      "Validation MSE: 0.069312\n",
      "Loss:  0.0697040855884552\n",
      "epoch: 6900\n",
      "Validation MSE: 0.069308\n",
      "Loss:  0.10777738690376282\n",
      "epoch: 7000\n",
      "Validation MSE: 0.069305\n",
      "Loss:  0.10793301463127136\n",
      "epoch: 7100\n",
      "Validation MSE: 0.069305\n",
      "Loss:  0.11475750803947449\n",
      "epoch: 7200\n",
      "Validation MSE: 0.069294\n",
      "Loss:  0.0928773507475853\n",
      "epoch: 7300\n",
      "Validation MSE: 0.069282\n",
      "Loss:  0.11694255471229553\n",
      "epoch: 7400\n",
      "Validation MSE: 0.069271\n",
      "Loss:  0.10468392074108124\n",
      "epoch: 7500\n",
      "Validation MSE: 0.069262\n",
      "Loss:  0.1294131726026535\n",
      "epoch: 7600\n",
      "Validation MSE: 0.069250\n",
      "Loss:  0.09380937367677689\n",
      "epoch: 7700\n",
      "Validation MSE: 0.069239\n",
      "Loss:  0.13872382044792175\n",
      "epoch: 7800\n",
      "Validation MSE: 0.069235\n",
      "Loss:  0.10906823724508286\n",
      "epoch: 7900\n",
      "Validation MSE: 0.069223\n",
      "Loss:  0.10255130380392075\n",
      "epoch: 8000\n",
      "Validation MSE: 0.069211\n",
      "Loss:  0.11522671580314636\n",
      "epoch: 8100\n",
      "Validation MSE: 0.069205\n",
      "Loss:  0.09777731448411942\n",
      "epoch: 8200\n",
      "Validation MSE: 0.069194\n",
      "Loss:  0.06973853707313538\n",
      "epoch: 8300\n",
      "Validation MSE: 0.069185\n",
      "Loss:  0.09178093075752258\n",
      "epoch: 8400\n",
      "Validation MSE: 0.069177\n",
      "Loss:  0.09954522550106049\n",
      "epoch: 8500\n",
      "Validation MSE: 0.069167\n",
      "Loss:  0.11006281524896622\n",
      "epoch: 8600\n",
      "Validation MSE: 0.069161\n",
      "Loss:  0.13104292750358582\n",
      "epoch: 8700\n",
      "Validation MSE: 0.069149\n",
      "Loss:  0.12091781198978424\n",
      "epoch: 8800\n",
      "Validation MSE: 0.069145\n",
      "Loss:  0.09165720641613007\n",
      "epoch: 8900\n",
      "Validation MSE: 0.069134\n",
      "Loss:  0.13814398646354675\n",
      "epoch: 9000\n",
      "Validation MSE: 0.069133\n",
      "Loss:  0.1335662305355072\n",
      "epoch: 9100\n",
      "Validation MSE: 0.069121\n",
      "Loss:  0.13280101120471954\n",
      "epoch: 9200\n",
      "Validation MSE: 0.069114\n",
      "Loss:  0.09856555610895157\n",
      "epoch: 9300\n",
      "Validation MSE: 0.069113\n",
      "Loss:  0.09786171466112137\n",
      "epoch: 9400\n",
      "Validation MSE: 0.069097\n",
      "Loss:  0.10769014805555344\n",
      "epoch: 9500\n",
      "Validation MSE: 0.069085\n",
      "Loss:  0.09415344893932343\n",
      "epoch: 9600\n",
      "Validation MSE: 0.069079\n",
      "Loss:  0.09220476448535919\n",
      "epoch: 9700\n",
      "Validation MSE: 0.069068\n",
      "Loss:  0.12288899719715118\n",
      "epoch: 9800\n",
      "Validation MSE: 0.069055\n",
      "Loss:  0.08038681745529175\n",
      "epoch: 9900\n",
      "Validation MSE: 0.069041\n",
      "Loss:  0.09576131403446198\n"
     ]
    }
   ],
   "source": [
    "train(model, num_epochs, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
